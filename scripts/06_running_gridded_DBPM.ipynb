{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b0fc6a1-968c-467f-9a6c-5d15003d2364",
   "metadata": {},
   "source": [
    "# Running gridded DBPM run\n",
    "**Author**: Denisse Fierro Arcos  \n",
    "**Date**: 2025-01-13  \n",
    "\n",
    "Once all necessary inputs have been processed and fishing parameters calculated, we can finally run the gridded version of DBPM.  \n",
    "\n",
    "Note that the temporal range of DBPM inputs needs to be taken into account before running the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f83d314-52dc-40d2-a95c-cac096a6a101",
   "metadata": {},
   "source": [
    "## Loading relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deff782b-529b-4f09-86ba-020ad634a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/g/data/vf71/la6889/lme_scale_calibration_ISMIP3a/new_workflow/')\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import useful_functions as uf\n",
    "import dask\n",
    "from distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bd8f5a-65ee-4ac2-b1f5-603eebe44715",
   "metadata": {},
   "source": [
    "## Start a cluster for parallelisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b17868f-906d-490c-b4ca-b5965141ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(threads_per_worker = 1)\n",
    "#Do not print warning about large chunks\n",
    "# dask.config.set({'array.slicing.split_large_chunks': False})\n",
    "# #Reduce size of chunks to 100 MB\n",
    "# dask.config.set({'array.chunk-size': '100 MB'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363c2ac5-dd94-499f-a499-5d8fc23f27f5",
   "metadata": {},
   "source": [
    "## Defining basic variables to run gridded DBPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "771407fd-01ca-43e2-9cba-fd1e58d0835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name of region and model resolution\n",
    "region = 'fao-88'\n",
    "model_res = '1deg'\n",
    "\n",
    "#Defining input and output folders\n",
    "base_folder = '/g/data/vf71/la6889/dbpm_inputs/west_antarctica/'\n",
    "gridded_folder = os.path.join(base_folder, 'gridded_params', model_res)\n",
    "out_folder = os.path.join(base_folder, 'run_fishing', model_res)\n",
    "#If output folder does not exist, it will create it\n",
    "os.makedirs(out_folder, exist_ok = True) \n",
    "\n",
    "## If starting DBPM run from a specific time step ----\n",
    "# Character: Year and month from when DBPM initialisation values should be loaded\n",
    "# If starting model for the first time, it should be set to None\n",
    "init_time = '1960-12'\n",
    "# init_time = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2507935e-32cf-40bb-b68e-60982b9d6739",
   "metadata": {},
   "source": [
    "## Loading fixed DBPM parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01f5e881-ab0c-47e4-a369-7169a0a1a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_fixed = uf.loading_dbpm_inputs(gridded_folder)\n",
    "#Adding additional fixed DBPM parameters to dataset\n",
    "#Depth\n",
    "depth = xr.open_zarr(glob(os.path.join(base_folder, 'gridded',\n",
    "                                       model_res, '*obsclim_deptho_*'))[0])['deptho']\n",
    "ds_fixed['depth'] = depth\n",
    "#Size bins in log10\n",
    "log10_size_bins_mat = xr.open_zarr('outputs/log10_size_bins_matrix.zarr/')['size_bins']\n",
    "ds_fixed['log10_size_bins_mat'] = log10_size_bins_mat\n",
    "#Removing datarrays added to fixed inputs\n",
    "del depth, log10_size_bins_mat\n",
    "\n",
    "#Scatter fixed dataset across workers\n",
    "# ds_fixed_scattered = client.scatter(ds_fixed)\n",
    "# #Complete scattering before use (\"dask future\")\n",
    "# ds_fixed_fut = ds_fixed_scattered.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134d0b5a-808f-4700-80b3-0cdd8ffc023d",
   "metadata": {},
   "source": [
    "## Loading DBPM initialisation variables\n",
    "These variables include biomass for `predators` and `detritivores`, as well as `detritus`.  \n",
    "  \n",
    "Note that to initialise DBPM from the beginning, you wil need to load the data produced in [05_setup_gridded_DBPM](new_workflow/05_setup_gridded_DBPM.ipynb). Additionally, the `init_time` variable above must be set to `None`.  \n",
    "  \n",
    "If DBPM needs to be started from a specific time, the `init_time` variable must be set to the year-month before DBPM is meant to began. For example, if you want to restart DBPM from `1900-01`, then the `predators`, `detritivores`, and `detritus` data should be from `1899-12` needs to be loaded. Note that these data should be in the `out_folder` as they should be have produced by a previous DBPM run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bbd604e-7bdc-4bcd-8835-7f2d4f9034ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If no 'init_time' is provided, model runs from the beginning\n",
    "if init_time is None:\n",
    "    predators = xr.open_zarr(\n",
    "        glob(os.path.join(gridded_folder, 'predators_*'))[0])['predators'] \n",
    "    detritivores = xr.open_zarr(\n",
    "        glob(os.path.join(gridded_folder, 'detritivores_*'))[0])['detritivores']\n",
    "    detritus = xr.open_zarr(\n",
    "        glob(os.path.join(gridded_folder, 'detritus_*'))[0])['detritus']\n",
    "#If 'init_time' is provided, model runs from 'init_time'\n",
    "else:\n",
    "    predators = xr.open_dataarray(\n",
    "        glob(os.path.join(out_folder, f'predators_*_{init_time}.nc'))[0])\n",
    "    detritivores = xr.open_dataarray(\n",
    "        glob(os.path.join(out_folder, f'detritivores_*_{init_time}.nc'))[0])\n",
    "    detritus = xr.open_dataarray(\n",
    "        glob(os.path.join(out_folder, f'detritus_*_{init_time}.nc'))[0])\n",
    "\n",
    "#Create dataset for predator, detritivores and detritus initialisation data\n",
    "ds_init = xr.Dataset(data_vars = {'predators': predators, \n",
    "                                  'detritivores': detritivores, \n",
    "                                  'detritus': detritus})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b71f8-083c-45e6-beac-9defcf89f925",
   "metadata": {},
   "source": [
    "## Loading dynamic variables\n",
    "These refer to all variables that change along the time dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4a6c5c0-5bc1-425f-a55b-ebd0091db5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spinup data is loaded if init_time is None or if the init_time year is less than 1960\n",
    "if init_time is None or pd.Timestamp(init_time).year < 1960:\n",
    "    #Intercept plankton size spectrum\n",
    "    ui0 = xr.open_zarr(glob(os.path.join(gridded_folder, 'ui0_spinup*'))[0])['ui0']\n",
    "    #Slope plankton size spectrum\n",
    "    slope = xr.open_zarr(glob(os.path.join(base_folder, 'gridded', \n",
    "                                           model_res, '*spinup_slope_*'))[0])['slope']\n",
    "    #Temperature effects\n",
    "    pel_tempeffect = xr.open_zarr(glob(\n",
    "        os.path.join(gridded_folder, 'pel-temp-eff_spinup*'))[0])['pel_temp_eff']\n",
    "    ben_tempeffect = xr.open_zarr(glob(\n",
    "        os.path.join(gridded_folder, 'ben-temp-eff_spinup*'))[0])['ben_temp_eff']\n",
    "    #Sinking rate\n",
    "    sinking_rate = xr.open_zarr(glob(os.path.join(base_folder, 'gridded', model_res,\n",
    "                                                  '*_spinup_er_*'))[0])['export_ratio']\n",
    "    # Loading effort\n",
    "    effort = xr.open_zarr(glob(os.path.join(gridded_folder, 'effort_spinup*'))[0])['effort']\n",
    "#Spinup data plus obsclim are loaded if init_time is 1960\n",
    "elif pd.Timestamp(init_time).year == 1960:\n",
    "    exp = ['spinup', 'obsclim']\n",
    "    #Intercept plankton size spectrum\n",
    "    ui0 = xr.open_mfdataset(glob(os.path.join(gridded_folder, 'ui0_*')), \n",
    "                            engine = 'zarr')['ui0']\n",
    "    #Slope plankton size spectrum\n",
    "    slope = xr.open_mfdataset([f for ex in exp for f in glob(\n",
    "        os.path.join(base_folder, 'gridded', model_res, f'*{ex}_slope_*'))], \n",
    "                              engine = 'zarr')['slope']\n",
    "    #Temperature effects\n",
    "    pel_tempeffect = xr.open_mfdataset(glob(os.path.join(gridded_folder, 'pel-temp-eff_*')),\n",
    "                                       engine = 'zarr')['pel_temp_eff']\n",
    "    ben_tempeffect = xr.open_mfdataset(glob(os.path.join(gridded_folder, 'ben-temp-eff_*')),\n",
    "                                       engine = 'zarr')['ben_temp_eff']\n",
    "    #Sinking rate\n",
    "    sinking_rate = xr.open_mfdataset([f for ex in exp for f in glob(\n",
    "        os.path.join(base_folder, 'gridded', model_res, f'*{ex}_er_*'))], \n",
    "                                     engine = 'zarr')['export_ratio']\n",
    "    # Loading effort\n",
    "    effort = xr.open_mfdataset(glob(os.path.join(gridded_folder, 'effort_*')),\n",
    "                               engine = 'zarr')['effort']\n",
    "#Obsclim data loaded from 1961 onwards:\n",
    "else:\n",
    "    #Intercept plankton size spectrum\n",
    "    ui0 = xr.open_zarr(glob(os.path.join(gridded_folder, 'ui0_[0-9]*'))[0])['ui0']\n",
    "    #Slope plankton size spectrum\n",
    "    slope = xr.open_zarr(glob(os.path.join(base_folder, 'gridded', \n",
    "                                           model_res, '*obsclim_slope_*'))[0])['slope']\n",
    "    #Temperature effects\n",
    "    pel_tempeffect = xr.open_zarr(glob(\n",
    "        os.path.join(gridded_folder, 'pel-temp-eff_[0-9]*'))[0])['pel_temp_eff']\n",
    "    ben_tempeffect = xr.open_zarr(glob(\n",
    "        os.path.join(gridded_folder, 'ben-temp-eff_[0-9]*'))[0])['ben_temp_eff']\n",
    "    #Sinking rate\n",
    "    sinking_rate = xr.open_zarr(glob(os.path.join(base_folder, 'gridded', model_res,\n",
    "                                                  '*_obsclim_er_*'))[0])['export_ratio']\n",
    "    # Loading effort\n",
    "    effort = xr.open_zarr(glob(os.path.join(gridded_folder, 'effort_[0-9]*'))[0])['effort']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8c2d78-efb7-4db9-b79e-b5c23ac6c071",
   "metadata": {},
   "source": [
    "### Optional: If running the model at specific time step\n",
    "As above, if DBPM needs to be restarted from a specific timestep, then data for dynamic variables need to be subsetted so the first time included is the same as the timestep that DBPM is being started from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0f225362-358f-4692-a4ec-143c93c82eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If running from a specific point in time, then data is subsetted from the month after\n",
    "#init_time\n",
    "if init_time is not None:\n",
    "    #Timestep from when to restart DBPM \n",
    "    subset_time = (pd.Timestamp(init_time)+pd.DateOffset(months = 1)).strftime('%Y-%m')\n",
    "    #Timestep from when to add init effort data\n",
    "    effort_time = (pd.Timestamp(init_time)+pd.DateOffset(months = 2)).strftime('%Y-%m')\n",
    "    \n",
    "    #Subset data from timestep above until the end of the available data\n",
    "    ui0 = ui0.sel(time = slice(subset_time, None))\n",
    "    slope = slope.sel(time = slice(subset_time, None))\n",
    "    pel_tempeffect = pel_tempeffect.sel(time = slice(subset_time, None))\n",
    "    ben_tempeffect = ben_tempeffect.sel(time = slice(subset_time, None))\n",
    "    sinking_rate = sinking_rate.sel(time = slice(subset_time, None))\n",
    "    #Load effort for time step DBPM starts\n",
    "    e_start = xr.open_dataarray(glob(os.path.join(out_folder, \n",
    "                                                  f'effort_*_{subset_time}.nc'))[0])\n",
    "    #Subset effort data from the timestep after DBPM restart \n",
    "    effort = effort.sel(time = slice(effort_time, None))\n",
    "    #Combine both data arrays\n",
    "    effort = xr.concat([e_start, effort], dim = 'time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cb41ec-215a-49e4-849b-8169a7613e11",
   "metadata": {},
   "source": [
    "Create a single dataset for dynamic inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "113baa5e-9455-4cc0-bfb3-a1c86cb05115",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dynamic = xr.Dataset(data_vars = {'ui0': ui0, 'slope': slope,\n",
    "                                     'pel_tempeffect': pel_tempeffect,\n",
    "                                     'ben_tempeffect': ben_tempeffect, \n",
    "                                     'sinking_rate': sinking_rate,\n",
    "                                     'effort': effort}).isel(time = slice(0, 4))\n",
    "#Scatter initialisation dataset across workers\n",
    "# ds_dynamic_scattered = client.scatter(ds_dynamic)\n",
    "# #Complete scattering before use (\"dask future\")\n",
    "# ds_dynamic_fut = ds_dynamic_scattered.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53920e6f-d3bd-4b7a-821e-173864e83d39",
   "metadata": {},
   "source": [
    "## Running gridded DBPM\n",
    "Note that this may take several hours depending on computing resources, as well as the temporal and spatial range of the modelled area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a474be3e-f21d-4020-86b7-3b82ac17031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "uf.gridded_sizemodel(gridded_folder, ds_fixed, ds_init, ds_dynamic, \n",
    "                     region = region, model_res = model_res, \n",
    "                     out_folder = out_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3-24.07]",
   "language": "python",
   "name": "conda-env-analysis3-24.07-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
